{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repaso de teoría de probabilidad\n",
    "\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://storage.needpix.com/rsynced_images/bayesian-2889576_1280.png\" width=\"200px\" height=\"180px\" />\n",
    "\n",
    "> La incertidumbre es un aspecto inevitable en la mayoría de aplicaciones; incluso, el debate si el mundo es determinista o estocástico es un debate abierto. Aún en el supuesto que el mundo siguiera un comportamiento determinista, las fuentes de incertidumbre están ahí, ya sea por una medición ruidosa, que los datos sean finitos, entre otros.\n",
    "\n",
    "> En este sentido, es de vital importancia modelar esta incertidumbre dentro de los fenómenos de interés. Es por esto que los modelos que se consideran en este curso (y en gran parte de sus carreras profesionales) son probabilísticos.\n",
    "\n",
    "> Por esa razón haremos un breve repaso de los conceptos que necesitaremos de teoría de probabilidad.\n",
    "\n",
    "> **Objetivos:**\n",
    "> - Repasar definiciones y resultados básicos de teoría de probabilidad.\n",
    "\n",
    "\n",
    "> **Referencias:**\n",
    "> \n",
    "> - Pattern Recognition and Machine Learning, by Christopher M. Bishop. Cap 1.2.\n",
    "> - Bayesian Reasoning and Machine Learning by David Barber. Cap. 1.\n",
    "> - Probabilistic Graphical Models: Principles and Techniques, by Daphne Koller and Nir Friedman. Cap. 2.\n",
    "> - https://ocw.mit.edu/courses/18-05-introduction-to-probability-and-statistics-spring-2022/mit18_05_s22_probability.pdf\n",
    "\n",
    "\n",
    "<p style=\"text-align:right;\"> Imagen recuperada de: https://storage.needpix.com/rsynced_images/bayesian-2889576_1280.png.</p>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Probabilidad vs Estadística\n",
    "\n",
    "Los campos de probabilidad y estadística están profundamente conectados. Incluso es probable que ustedes no hagan diferenciación alguna entre una u otra. Esto es debido a que todos los elementos estadísticos están formulados con base en elementos probabilísticos.\n",
    "\n",
    "- La proabilidad es el estudio lógico y matemático formal de la incertidumbre. En ese sentido existen algunas *reglas* que debemos seguir (y estudiaremos en este cuaderno), y a través de ellas las respuestas se siguen *lógicamente*. Acá hay que aclarar que los cómputos que resultan al seguir estas reglas pueden ser sumamente complejos.\n",
    "\n",
    "> Por ejemplo, un problema probabilístico es: Se tiene una moneda justa (igual probabilidad de cara/sello). Se lanza la moneda 100 veces. ¿Cuál es la probabilidad de obtener 60 o más caras? Solo hay una respuesta correcta a este problema (al rededor de 0.028444), y aprenderemos a calcularla.\n",
    "\n",
    "- En estadística, aplicamos probabilidad para obtener conclusiones de los datos.\n",
    "\n",
    "> Por ejemplo, un problema probabilístico es: Se tiene una moneda de procedencia desconocida. Para investigar si es una moneda justa, la tiramos 100 veces y contamos el número de caras. Digamos que contamos 60 caras. Nuestro trabajo como estadísticos es obtener conclusiones (inferencia) de estos datos. Hay varias formas de proceder, las cuales dependen del tipo de conclusión que se desea obtener y los cálculos que se usan para obtener la conclusión.\n",
    "\n",
    "\n",
    "### 0.1 Interpretación frecuentista vs. Bayesiana\n",
    "\n",
    "Respecto al último comentario del párrafo anterior, existen dos prominentes escuelas de estadística, que se conflictúan entre sí de vez en vez. Sus enfoques tienen raices en diferentes interpretaciones del significado de probabilidad.\n",
    "\n",
    "- **La interpretación frecuentista** dice que la probabilidad mida la frecuencia de diferentes resultados de un experimento. Por ejemplo, decir que una moneda tiene un 50% de probabilidad de caer cara, significa que si la tiramos muchas veces, entonces esperamos que al rededor de la mitad de los tiros sean cara.\n",
    "\n",
    "- **La interpretación Bayesiana** dice que la probabilidad es un concepto abstracto que mide el estado de conocimiento o grado de confianza que se tiene en una proposición. En la práctica, los Bayesianos no asignan un solo valor de probabilidad a que la moneda caiga cara. Ellos consideran un rango de valores, cada uno con su propia probabilidad de ser cierto.\n",
    "\n",
    "![blowing-head](https://th.bing.com/th/id/OIP.T7MrIm2KwGXrrpJp_iGIYAAAAA?rs=1&pid=ImgDetMain)\n",
    "\n",
    "Históricamente, el enfoque frecuentista ha dominado las áreas de biología, medicina, salud pública y ciencias sociales. Por otra parte, el enfoque Bayesiano ha resurgido en nuestra era de computadoras poderosas y big data. Tranquilos, no se trata de dos bandos entre los que hay que elegir. Podemos usar ambos enfoques en maneras complementarias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción a probabilidad\n",
    "\n",
    "La teoría de probabilidad nos dota de las bases centrales para cuantificar y manipular la incertidumbre. Esto es bastante importante, teniendo en cuenta que *aunque en casos simples (dados, rifas, moneda al aire) podemos razonar con la incertidumbre de manera intuitiva*, en escenarios más complejos (muchas variables interactuando entre sí) la intuición se queda corta y se hace necesaria una manera formal para extender la intuición.\n",
    "\n",
    "Para introducir algunos conceptos básicos de probabilidad, usamos un ejemplo bastante sencillo (tomado de Pattern Recognition and Machine Learning, by Christopher M. Bishop):\n",
    "\n",
    "Supongamos que tenemos dos cajas, una <font color=red>roja</font> y una <font color=blue>azul</font>. En la caja roja tenemos <font color=green>2 manzanas</font> y <font color=orange>6 naranjas</font>, y en la caja azul tenemos <font color=green>3 manzanas</font> y <font color=orange>1 naranja</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caja roja\n",
    "plt.plot([0, 0, 3, 3], [3, 0, 0, 3], 'r', lw=5)\n",
    "# 6 naranjas\n",
    "plt.plot([0.5, 0.5, 1.5, 1.5, 2.5, 2.5],\n",
    "         [0.5, 1., 0.5, 1., 0.5, 1.], 'o', ms=30, c='tab:orange')\n",
    "# 2 manzanas\n",
    "plt.plot([0.5, 1.5],\n",
    "         [1.5, 1.5], 'o', ms=30, c='tab:green')\n",
    "# Caja azul\n",
    "plt.plot([4, 4, 7, 7], [3, 0, 0, 3], 'b', lw=5)\n",
    "# 3 manzanas\n",
    "plt.plot([4.5, 5.5, 6.5],\n",
    "         [0.5, 0.5, 0.5], 'o', ms=30, c='tab:green')\n",
    "# 1 naranjas\n",
    "plt.plot([5.5],\n",
    "         [1.], 'o', ms=30, c='tab:orange')\n",
    "plt.xticks([])\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imaginemos que seleccionamos aleatoriamente una de las cajas, 40% de las veces seleccionamos la caja <font color=blue>azul</font> y el 60% de las veces seleccionamos la caja <font color=red>roja</font>, y de la caja seleccionada tomamos aleatoriamente una fruta, con igual probabilidad de seleccionar cualquier cualquier elemento dentro de la caja. Finalmente devolvemos la fruta a la caja original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la identidad de la caja que seleccionamos como una **variable aleatoria** (V.A.), la cual denotamos como $C$, y puede tomar los valores <font color=red>r</font> y <font color=blue>a</font>.\n",
    "\n",
    "Similarmente, definimos la identidad de la fruta seleccionada como una V.A., denotada por $F$, y que puede tomar los valores <font color=green>m</font> y <font color=orange>n</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo definimos la probabilidad de un evento?\n",
    "\n",
    "Definiremos la probabilidad de un evento como **la fracción de veces que ocurre este evento entre la cantidad de repeticiones, cuando el número de repeticiones tiende a infinito**.\n",
    "\n",
    "En este sentido, las probabilidades de seleccionar:\n",
    "\n",
    "* <font color=red>La caja roja</font>: $p(C=r) = \\frac{6}{10} = 0.6$\n",
    "* <font color=blue>La caja azul azul</font>: $p(C=a) = \\frac{4}{10} = 0.4$\n",
    "\n",
    "A la luz de esta definición se intuyen dos reglas muy importantes:\n",
    "\n",
    "1. Cualquier probabilidad debe ser un número en el intervalo $[0,1]$:\n",
    "   \n",
    "   $$\n",
    "   0 \\leq p(X=x) \\leq 1.\n",
    "   $$\n",
    "   \n",
    "2. Si los eventos son mutuamente excluyentes (la caja no puede ser roja y azul a la vez, por lo menos en este ejemplo), y son exhaustivos (la caja solo puede ser roja o azul), las probabilidades suman 1:\n",
    "\n",
    "   $$\n",
    "   \\sum_{x} p(X=x) = 1.\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preguntas plausibles serían entonces:\n",
    "\n",
    "- *¿Cuál es la probabilidad de seleccionar una manzana?*\n",
    "- *Dado que elegimos una naranja, ¿Cuál es la probabilidad que la caja haya sido la azul?*\n",
    "\n",
    "Notemos que estas probabilidades no las conocemos de antemano. Incluso, notemos que son probabilidades que involucran más de una variable. Sin embargo, tenemos la información necesaria para **inferir estas probabilidades**, no sin antes conocer la regla de la suma (**marginalización**), y la regla del producto (**regla de la cadena**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtener estas reglas en nuestro modo intuitivo de estudiar probabilidad, consideremos el caso más general en que tenemos dos V.A. $X$ y $Y$, las cuales pueden tomar los valores $x^i$ para $i=0,\\dots,s$ y $y^j$ para $j=0,\\dots,t$.\n",
    "\n",
    "Supongamos que, de un total de $N$ repeticiones, \n",
    "\n",
    "- en $n_{ij}$ ocasiones obtuvimos $X=x^i$ y $Y=y^j$;\n",
    "- en $c_{i}$ ocasiones obtuvimos $X=x^i$, sin importar el valor de $Y$;\n",
    "- en $r_{j}$ ocasiones obtuvimos $Y=y^j$, sin importar el valor de $X$;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Malla\n",
    "plt.axhline(y=0, c='r', lw=5)\n",
    "plt.axhline(y=1, c='r', lw=5)\n",
    "plt.axhline(y=2, c='r', lw=5)\n",
    "plt.axhline(y=3, c='r', lw=5)\n",
    "plt.axvline(x=0, c='r', lw=5)\n",
    "plt.axvline(x=1, c='r', lw=5)\n",
    "plt.axvline(x=2, c='r', lw=5)\n",
    "plt.axvline(x=3, c='r', lw=5)\n",
    "plt.axvline(x=4, c='r', lw=5)\n",
    "plt.text(-0.3, 1.5, '$y^j$', size=18)\n",
    "plt.text(2.5, -0.3, '$x^i$', size=18)\n",
    "plt.text(2.6, 1.7, '$n_{ij}$', size=18)\n",
    "plt.text(2.5, 3.3, '$c_i$', size=18)\n",
    "plt.arrow(2.5, 0.1, 0, 3., width=0.1, length_includes_head=True)\n",
    "plt.text(4.3, 1.5, '$r_j$', size=18)\n",
    "plt.arrow(0.1, 1.5, 4., 0, width=0.1, length_includes_head=True)\n",
    "plt.axis([0, 4, 0, 3])\n",
    "plt.xticks([])\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De nuestra definición de probabilidad, tenemos que (suponiendo que $N \\to \\infty$):\n",
    "\n",
    "1. La **probabilidad conjunta** de que $X=x^i$ y $Y=y^j$ es:\n",
    "\n",
    "   $$\n",
    "   p(X=x^i, Y=y^j) = \\frac{n_{ij}}{N}.\n",
    "   $$\n",
    "\n",
    "2. La **probabilidad marginal** de que $X=x^i$ sin importar el valor de Y es;\n",
    "   \n",
    "   $$\n",
    "   p(X=x^i) = \\frac{c_{i}}{N}.\n",
    "   $$\n",
    "   \n",
    "   Notemos que $c_i = \\sum_j n_{ij}$, y en este sentido podemos establecer la regla de la suma (**marginalización**):\n",
    "   \n",
    "   $$\n",
    "   p(X=x^i) = \\sum_{j=0}^{t} p(X=x^i, Y=y^j).\n",
    "   $$\n",
    "   \n",
    "   Similarmente, podemos definir la probabilidad marginal $p(Y=y^j)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Si en lugar de considerar todos los posibles repeticiones, consideramos solo aquellas para las que $X=x^i$, entonces la fracción de dichas repeticionesoara kas cuales $Y=y^j$, la conocemos como **probabilidad condicional** de $Y=y^j$ dado $X=x^i$, y la escribimos como:\n",
    "\n",
    "   $$\n",
    "   p(Y=y^j | X=x^i) = \\frac{n_{ij}}{c_i}.\n",
    "   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Malla\n",
    "plt.axhline(y=0, c='r', lw=5)\n",
    "plt.axhline(y=1, c='r', lw=5)\n",
    "plt.axhline(y=2, c='r', lw=5)\n",
    "plt.axhline(y=3, c='r', lw=5)\n",
    "plt.axvline(x=0, c='r', lw=5)\n",
    "plt.axvline(x=1, c='r', lw=5)\n",
    "plt.axvline(x=2, c='r', lw=5)\n",
    "plt.axvline(x=3, c='r', lw=5)\n",
    "plt.axvline(x=4, c='r', lw=5)\n",
    "plt.text(-0.3, 1.5, '$y_j$', size=18)\n",
    "plt.text(2.5, -0.3, '$x_i$', size=18)\n",
    "plt.text(2.5, 1.5, '$n_{ij}$', size=18)\n",
    "plt.text(2.5, 3.3, '$c_i$', size=18)\n",
    "plt.arrow(2.5, 0., 0, 3., width=1, length_includes_head=False, alpha=0.5)\n",
    "plt.axis([0, 4, 0, 3])\n",
    "plt.xticks([])\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que la probabilidad conjunta, la podemos escribir como:\n",
    "\n",
    "$$\n",
    "p(X=x^i, Y=y^j) = \\frac{n_{ij}}{N} = \\frac{n_{ij}}{c_i} \\frac{c_{i}}{N} = p(Y=y^j | X=x^i) p(X=x^i),\n",
    "$$\n",
    "\n",
    "dando lugar a la regla del producto (**regla de la cadena**), y dando lugar a la definición de probabilidad condicional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **probabilidad condicional** de una V.A. $Y$ condicionada a que conocemos la V.A. $X$ (la probabilidad de $Y$ dado $X$), se define como:\n",
    "\n",
    "$$\n",
    "p(Y|X) = \\frac{p(X, Y)}{p(X)},\n",
    "$$\n",
    "\n",
    "siempre que $p(X)>0$. Si $p(X)=0$, entonces $p(Y|X)$ no está definida.\n",
    "\n",
    "La probabilidad condicional es una distribución de probabilidad válida, en el sentido que:\n",
    "\n",
    "- $0 \\leq p(Y|X) \\leq 1$, y\n",
    "- $\\sum_{Y} p(Y|X) = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo.**\n",
    "\n",
    "Supongamos que se tira una moneda justa 3 veces.\n",
    "\n",
    "1. ¿Cuál es la probabilidad de obtener 3 caras?\n",
    "\n",
    "<details>\n",
    "  <summary>Descubrir</summary>\n",
    "  \n",
    "Tenemos los siguientes posibles resultados, donde C denota cara y S denota sello:\n",
    "$$\n",
    "\\{CCC, CCS, CSC, SCC, CSS, SCS, SSC, SSS\\}\n",
    "$$\n",
    "\n",
    "Todos los resultados son igualmente probables, de modo que $p(\\text{3 caras}) = \\frac{1}{8}$.\n",
    "</details>\n",
    "\n",
    "2. Suponga que sabemos que el primer resultado del tiro fue cara. Con esta información, ¿Cómo podemos calcular la probabilidad de obtener 3 caras?\n",
    "\n",
    "<details>\n",
    "  <summary>Descubrir</summary>\n",
    "  \n",
    "El conjunto de resultados se reduce a:\n",
    "$$\n",
    "\\{CCC, CCS, CSC, CSS\\}\n",
    "$$\n",
    "\n",
    "Todos los resultados son igualmente probables, de modo que $p(\\text{3 caras} | \\text{cara en primer tiro}) = \\frac{1}{4}$.\n",
    "\n",
    "Notemos que, a partir de la definición:\n",
    "\n",
    "$$\n",
    "p(\\text{3 caras} | \\text{cara en primer tiro}) = \\frac{p(\\text{3 caras}, \\text{cara en primer tiro})}{p(\\text{cara en primer tiro})} = \\frac{p(\\text{3 caras})}{p(\\text{cara en primer tiro})} = \\frac{1/8}{1/2} = \\frac{1}{4}\n",
    "$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observación en la notación.** Originalmente, escribimos $p(X=x)$ para referirnos al evento en que la variable aleatoria $X$ toma el valor $x$. Esta notación elimina \"incertidumbre\", pero es algo engorrosa. De manera que, en adelante, escribiremos simplemente $p(x)$ para referirnos a la probabilidad del evento $x$, y $p(X)$ para referirnos a la distribución de probabilidad de la V.A. $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la definición de probabilidad condicional, se desprende automáticamente lo que conocemos como **la regla de la cadena** en probabilidad:\n",
    "\n",
    "$$\n",
    "p(X, Y) = p(Y|X) p(X).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo a la mano la regla de la cadena, y la marginalización, podemos escribir **la regla de probabilidad total** como:\n",
    "\n",
    "$$\n",
    "p(X) = \\sum_Y p(X | Y) p(Y),\n",
    "$$\n",
    "\n",
    "y se puede entender como una constante de normalización para asegurar que la probabilidad condicional sea **una distribución de probabilidad válida**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> De manera que, a modo de resumen:\n",
    "> \n",
    ">  - $0 \\leq p(X) \\leq 1$: Definición de probabilidad (i)\n",
    ">  - $\\sum_X p(X) = 1$: Definición de probabilidad (ii)\n",
    ">  - $p(X) = \\sum_{Y} p(X, Y)$: Marginalización\n",
    ">  - $p(X, Y) = p(Y | X) p(X) = p(X | Y) p(Y)$: Regla de la cadena\n",
    ">  - $p(X) = \\sum_Y p(X | Y) p(Y)$: Probabilidad total\n",
    ">\n",
    "> La regla de la cadena, se puede extender al caso de $n$ V.A. como:\n",
    "> \n",
    "> $$\n",
    "  p(X_1, X_2, X_3, \\dots, X_n) = p(X_1)p(X_2 | X_1) p(X_3 | X_2, X_1) \\dots p(X_n | X_{n-1}, \\dots, X_1).\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ahora sí, respondamos las preguntas...\n",
    "\n",
    "- *¿Cuál es la probabilidad de seleccionar una manzana?*\n",
    " \n",
    "Primero que nada, las probabilidades que tenemos son:\n",
    "\n",
    "<details>\n",
    "  <summary>Descubrir</summary>\n",
    "  \n",
    "$$\n",
    "p(a) = 0.4 = \\frac{2}{5}, \\qquad p(r) = 0.6 = \\frac{3}{5}, \\qquad p(n | a) = \\frac{1}{4}, \\qquad p(m | a) = \\frac{3}{4}, \\qquad p(n | r) = \\frac{3}{4}, \\qquad p(m | r) = \\frac{1}{4}.\n",
    "$$\n",
    "\n",
    "En este sentido, y usando la regla de la probabilidad total:\n",
    "\n",
    "$$\n",
    "p(m) = p(m | a) p(a) + p(m | r) p(r) = \\frac{3}{4} \\times \\frac{2}{5} + \\frac{1}{4} \\times \\frac{3}{5} = \\frac{9}{20}.\n",
    "$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Dado que elegimos una naranja, ¿Cuál es la probabilidad que la caja haya sido la azul?*\n",
    "\n",
    "<details>\n",
    "  <summary>Descubrir</summary>\n",
    "  \n",
    "Ahora, podemos usar la regla de Bayes:\n",
    "\n",
    "$$\n",
    "p(a | n) = \\frac{p(n | a) p(a)}{p(n)}\n",
    "$$\n",
    "\n",
    "de donde ya conocemos $p(n | a)$ y la **previa** $p(a)$. Adicionalmente,\n",
    "\n",
    "$$\n",
    "p(n) = 1 - p(m) = \\frac{11}{20}.\n",
    "$$\n",
    "\n",
    "Por lo cual:\n",
    "\n",
    "$$\n",
    "p(a | n) = \\frac{1}{4} \\times \\frac{2}{5} \\times \\frac{20}{11} = \\frac{2}{11}\n",
    "$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La respuesta a la segunda pregunta es bastante interesante, y demuestra el proceso fundamental de incorporar evidencia en un problema. \n",
    "\n",
    "- Notemos que, antes de saber qué fruta elegimos, **la probabilidad previa** de elegir la caja azul es $p(a) = \\frac{4}{10}$.\n",
    "\n",
    "- Ahora, al incorporar la evidencia de que la fruta que elegimos fue una naranja, observamos que **la probabilidad posterior** de elegir la caja azul disminuyó considerablemente a $p(a | n) = \\frac{2}{11}$.\n",
    "\n",
    "- Lo anterior es intuitivo, dado que la proporción de naranjas es significativamente más alta en la caja roja ($p(n | r) = \\frac{3}{4}$) que en la caja azul ($p(n | a) = \\frac{1}{4}$).\n",
    "\n",
    "> Por esta bondad de añadir información de evidencia a nuestras inferencias es por lo que **la regla de Bayes es tan relevante.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepto de independencia\n",
    "\n",
    "Seguramente, desde sus cursos básicos de probabilidad en la licenciatura recordarán lo siguiente:\n",
    "\n",
    "> Dos V.A. $X$ y $Y$ son independiente si la distribución conjunta factoriza como el producto de marginales:\n",
    ">  \n",
    "> $$\n",
    "  p(X, Y) = p(X) p(Y).\n",
    "  $$\n",
    "  \n",
    "Ahora bien, la definición anterior es operativamente útil para establecer independencia, más no es para nada intuitiva en el sentido que no podemos relacionar la factorización con independencia. \n",
    "\n",
    "Incluso, hay casos que parecieran ser contraintuitivos, como el siguiente (Tomado de Bayesian Reasoning and Machine Learning by David Barber). Considere las V.A. binarias $X$ y $Y$, tales que:\n",
    "\n",
    "$$\n",
    "p(x^0, y^0) = 1, \\qquad p(x^0, y^1) = 0, \\qquad p(x^1, y^0) = 0, \\qquad p(x^1, y^1) = 0.\n",
    "$$\n",
    "\n",
    "Podríamos decir intuitiva y **erróneamente** que $X$ y $Y$ son dependientes porque ambas son iguales, e iguales a $0$ con probabilidad 1 (con toda certeza). \n",
    "\n",
    "Sin embargo, al marginalizar obtenemos que:\n",
    "\n",
    "$$\n",
    "p(x^0) = 1, \\qquad p(x^1) = 0, \\qquad p(y^0) = 1, \\qquad p(y^1)=0,\n",
    "$$\n",
    "\n",
    "y con esto es fácil ver que $p(X, Y) = p(X) p(Y)$ en todos los casos.\n",
    "\n",
    "**¿Qué significa entonces independencia?**\n",
    "\n",
    "Si movemos un poco las ecuaciones, y suponiendo que $p(X)>0$ y $p(Y)>0$, podemos ver que de la definición de probabilidad condicional:\n",
    "\n",
    "$$\n",
    "p(X | Y) = \\frac{p(X, Y)}{p(Y)} = \\frac{p(X) p(Y)}{p(Y)} = p(X)\n",
    "$$\n",
    "\n",
    "y similarmente\n",
    "\n",
    "$$\n",
    "p(Y | X) = p(Y).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Y esto qué? Bueno, pues vemos que si dos variables son independientes, **la evidencia sobre una de las variables no aporta ninguna información acerca de la otra**.\n",
    "\n",
    "En ese sentido, cuando hablamos de independencia en el contexto probabilístico, la pregunta que nos debemos hacer es **¿Conocer evidencia de una variable agrega información que no sepamos sobre otra?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo.**\n",
    "\n",
    "Se selecciona una carta de una baraja. Examinemos la independencia de 3 eventos:\n",
    "\n",
    "- $A$: la carta es un A's.\n",
    "- $C$: la carta es de corazón.\n",
    "- $R$: la carta es roja.\n",
    "\n",
    "<details>\n",
    "  <summary>Descubrir</summary>\n",
    "  \n",
    "1. Sabemos que\n",
    "  - $p(A)=\\frac{4}{52}=\\frac{1}{13}$ (hay 4 A's en una baraja),\n",
    "  - $p(A|C)=\\frac{1}{13}$ (hay una A de los 13 corazones).\n",
    "  Como $p(A)=p(A|C)$, entonces sacar una A es independiente de sacar un corazón.\n",
    "\n",
    "2. Similarmente,\n",
    "  - $p(A|R)=\\frac{2}{26}=\\frac{1}{13}$ (hay dos A's de las 26 cartas rojas).\n",
    "  Como $p(A)=p(A|R)$, entonces sacar una A es independiente de sacar una carta roja.\n",
    "\n",
    "3. ¿Qué pasa con $C$ y $R$? Tenemos que:\n",
    "  - $p(C)=\\frac{13}{52}=\\frac{1}{4}$\n",
    "  - $p(C|R)=\\frac{13}{26}=\\frac{1}{2}$\n",
    "  Por lo que sacar una carta roja no es independiente de sacar un corazón. Esto es intuitivo.\n",
    "\n",
    "Notar que el último caso, lo podríamos haber analizado en el otro sentido:\n",
    "  - $p(R)=\\frac{26}{52}=\\frac{1}{2}$\n",
    "  - $p(R|H)=\\frac{13}{13}=\\frac{1}{1}$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Independencia condicional.\n",
    "\n",
    "La independencia es una propiedad muy útil. Sin embargo, no es muy común encontrar eventos independientes cuando se analizan situaciones reales.\n",
    "\n",
    "Por otra parte, una situación más común es encontrar eventos independientes dado un evento adicional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, supongamos que un estudiante aplica a dos prácticas profesionales en las empresas $A$ y $B$ al mismo tiempo.\n",
    "\n",
    "Estas dos V.A., no son independientes en principio, pues saber que $A=a^1$ (el estudiante fue aceptado en la empresa $A$), aumenta la probabilidad de que $B=b^1$ y vice versa. \n",
    "\n",
    "Por otra parte, supongamos que las empresas obtienen tantos CVs de estudiantes, que deciden tomar la decisión de a quién aceptar con base en el promedio en la carrera, **únicamente**. \n",
    "\n",
    "Entonces si $X$ es la V.A. que representa el promedio de la carrera del estudiante, conocer $X$ **independiza** a las variables $A$ y $B$. Es decir\n",
    "\n",
    "$$\n",
    "p(A, B | X) = p(A| X) p(B| X),\n",
    "$$\n",
    "\n",
    "o equivalentemente,\n",
    "\n",
    "$$\n",
    "p(A | B, X) = p(A| X),\n",
    "$$\n",
    "\n",
    "o equivalentemente\n",
    "\n",
    "$$\n",
    "p(B | A, X) = p(B| X).\n",
    "$$\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teorema de Bayes\n",
    "\n",
    "El teorema de Bayes es un pilar fundamental de probabilidad y estadística. Aunque su derivación es sencilla, las imlicaciones que tiene son bastante poderosas.\n",
    "\n",
    "Dado que *la probabilidad conjunta es simétrica*, esto es $p(X, Y) = p(Y, X)$, de la definición de probabilidad condicional obtenemos que:\n",
    "\n",
    "$$\n",
    "p(Y|X) p(X) = p(X, Y) = p(Y, X) = p(X|Y) p(Y).\n",
    "$$\n",
    "\n",
    "De lo anterior, usando las igualdades de los extremos, obtenemos **la regla de Bayes**:\n",
    "\n",
    "$$\n",
    "p(Y | X) = \\frac{p(X | Y) p(Y)}{p(X)}.\n",
    "$$\n",
    "\n",
    "- La regla de Bayes nos dice cómo *invertir* probabilidades condicionales, es decir, nos permite encontrar $p(Y|X)$ a partir de $p(X|Y)$.\n",
    "- En la práctica, es común calcular $p(X)$ usando la regla de la probabilidad total:\n",
    "  $$\n",
    "  p(Y | X) = \\frac{p(X | Y) p(Y)}{p(X)} = \\frac{p(X | Y) p(Y)}{\\sum_{Y} p(X|Y)p(Y)} .\n",
    "  $$\n",
    "\n",
    "**Ejemplo.** Se tira una moneda 5 veces. Queremos analizar los eventos \n",
    "- F: el primer tiro es cara\n",
    "- T: todos los 5 tiros son cara.\n",
    "\n",
    "En particular $p(T | F)$.\n",
    "\n",
    "Es fácil observar que $p(F | T) = 1$. También, sabemos que $p(T) = \\left(\\frac{1}{2}\\right)^5 = \\frac{1}{32}$. Finalmente, $p(F) = \\frac{1}{2}$.\n",
    "\n",
    "Finalmente, aplicando la regla de Bayes:\n",
    "\n",
    "$$\n",
    "p(T | F) = \\frac{p(F | T) p(T)}{p(F)} = \\frac{1 \\cdot 1/32}{1/2} = \\frac{1}{16}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anuncios parroquiales\n",
    "\n",
    "### 1. Pueden hacer los puntos 1 y 2 de la tarea.\n",
    "### 2. Para los más visuales, les recomiendo este [material](https://seeing-theory.brown.edu/basic-probability/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#808080; background:#fff;\">\n",
    "Created with Jupyter by Esteban Jiménez Rodríguez.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
